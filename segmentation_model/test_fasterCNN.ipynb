{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import albumentations as A\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "import numpy as np\n",
    "from skimage import io, exposure\n",
    "import json\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import FasterRCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_transform():\n",
    "    return A.Compose([\n",
    "        A.Resize(512, 512),\n",
    "        ToTensorV2(),\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LungsAnnotationDataset(Dataset):\n",
    "    def __init__(self, image_dir, transforms=None):\n",
    "        super().__init__()\n",
    "        self.image_dir = image_dir\n",
    "        self.image_ids = os.listdir(image_dir)  # List all image files in the directory\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        image_id = self.image_ids[index]\n",
    "        image_path = os.path.join(self.image_dir, image_id)\n",
    "\n",
    "        image = Image.open(image_path)\n",
    "\n",
    "        if image.mode != 'RGB':\n",
    "            image = image.convert('RGB')\n",
    "\n",
    "        image = image.resize((512, 512))\n",
    "\n",
    "        image = np.array(image).astype(np.float32) / 255.0\n",
    "\n",
    "        h, w, c = image.shape\n",
    "\n",
    "        image = np.transpose(image, (2, 0, 1))  \n",
    "\n",
    "        image = torch.tensor(image)\n",
    "\n",
    "        return image, image_id  # Return image and image_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = LungsAnnotationDataset(image_dir='test_resized/', transforms=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    images, image_ids = zip(*batch)\n",
    "    images = [image.permute(1, 2, 0) if image.shape[0] != 3 else image for image in images]\n",
    "    return torch.stack(images), image_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_brands = {\n",
    "    0: 'Aortic enlargement',\n",
    "    1: 'Atelectasis',\n",
    "    2: 'Calcification',\n",
    "    3: 'Cardiomegaly',\n",
    "    4: 'Consolidation',\n",
    "    5: 'ILD',\n",
    "    6: 'Infiltration',\n",
    "    7: 'Lung Opacity',\n",
    "    8: 'Nodule/Mass',\n",
    "    9: 'Other lesion',\n",
    "    10: 'Pleural effusion',\n",
    "    11: 'Pleural thickening',\n",
    "    12: 'Pneumothorax',\n",
    "    13: 'Pulmonary fibrosis'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_loader = DataLoader(val_dataset, batch_size=5, shuffle=False, collate_fn=lambda x: list(zip(*x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes=15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_device():\n",
    "    device =  \"cpu\"\n",
    "    return device\n",
    "\n",
    "device=set_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=[], unexpected_keys=['roi_heads.box_predictor.fc.weight', 'roi_heads.box_predictor.fc.bias'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('x_ray_models/model_fasterRCNN_finetuned.pth', map_location=device), strict=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_predictions_to_json(model, val_data_loader, device, class_brands, filename='phase1_results.json'):\n",
    "    results = {}\n",
    "    \n",
    "    model.to(device)  # Ensure the model is on the same device as the inputs\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, image_ids in val_data_loader:  # Only images and image_ids\n",
    "            images = [image.to(device) for image in images]  # Move images to the same device\n",
    "\n",
    "            # Ensure images are in correct format (e.g., 3 channels, correct shape)\n",
    "            for idx, image in enumerate(images):\n",
    "                # print(f\"Image ID: {image_ids[idx]}, Shape: {image.shape}\")  # Print image shape\n",
    "                assert image.shape[0] == 3, f\"Expected 3 channels, but got {image.shape[0]} channels\"  # Confirm 3 channels\n",
    "                assert image.shape[1] == 512 and image.shape[2] == 512, f\"Expected (512, 512), but got {(image.shape[1], image.shape[2])}\"\n",
    "\n",
    "            outputs = model(images)  # Model and inputs are on the same device\n",
    "            \n",
    "            for i, output in enumerate(outputs):\n",
    "                image_id = image_ids[i]  # Use image_id directly\n",
    "                \n",
    "                pred_boxes = output['boxes']\n",
    "                labels_pred = output['labels']\n",
    "                scores = output['scores'].data.cpu().numpy()\n",
    "\n",
    "                # Filter predicted boxes based on confidence score\n",
    "                valid_indices = scores >= 0.6\n",
    "                boxes_pred = pred_boxes[valid_indices]\n",
    "                labels_pred = labels_pred[valid_indices]\n",
    "\n",
    "                # Convert results to list of dictionaries\n",
    "                pred_results = []\n",
    "                for box, label in zip(boxes_pred, labels_pred):\n",
    "                    box_np = box.detach().cpu().numpy().astype(int).tolist()  # Convert tensor to list\n",
    "                    class_name = class_brands.get(label.item(), 'Unknown')\n",
    "                    pred_results.append({\n",
    "                        'box': box_np,\n",
    "                        'class_label': class_name\n",
    "                    })\n",
    "\n",
    "                \n",
    "\n",
    "                results[image_id] = pred_results\n",
    "    \n",
    "    # Write results to a JSON file\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "\n",
    "    print(f\"Results saved to {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to phase1_results.json\n"
     ]
    }
   ],
   "source": [
    "save_predictions_to_json(model, val_data_loader, device, class_brands)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
