{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import model_selection\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import cv2\n",
    "from skimage import io, exposure\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import bbox_visualizer as bbv\n",
    "\n",
    "import pydicom\n",
    "from pydicom.pixel_data_handlers.util import apply_voi_lut\n",
    "from glob import glob\n",
    "from skimage import exposure\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import SequentialSampler\n",
    "import torchvision\n",
    "\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "from torchvision.ops import box_iou\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>class_name</th>\n",
       "      <th>class_id</th>\n",
       "      <th>rad_id</th>\n",
       "      <th>x_min</th>\n",
       "      <th>y_min</th>\n",
       "      <th>x_max</th>\n",
       "      <th>y_max</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50a418190bc3fb1ef1633bf9678929b3</td>\n",
       "      <td>No finding</td>\n",
       "      <td>14</td>\n",
       "      <td>R11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2332.0</td>\n",
       "      <td>2580.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21a10246a5ec7af151081d0cd6d65dc9</td>\n",
       "      <td>No finding</td>\n",
       "      <td>14</td>\n",
       "      <td>R7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2954.0</td>\n",
       "      <td>3159.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9a5094b2563a1ef3ff50dc5c7ff71345</td>\n",
       "      <td>Cardiomegaly</td>\n",
       "      <td>3</td>\n",
       "      <td>R10</td>\n",
       "      <td>0.332212</td>\n",
       "      <td>0.588613</td>\n",
       "      <td>0.794712</td>\n",
       "      <td>0.783818</td>\n",
       "      <td>2080.0</td>\n",
       "      <td>2336.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>051132a778e61a86eb147c7c6f564dfe</td>\n",
       "      <td>Aortic enlargement</td>\n",
       "      <td>0</td>\n",
       "      <td>R10</td>\n",
       "      <td>0.548611</td>\n",
       "      <td>0.257986</td>\n",
       "      <td>0.699219</td>\n",
       "      <td>0.353819</td>\n",
       "      <td>2304.0</td>\n",
       "      <td>2880.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>063319de25ce7edb9b1c6b8881290140</td>\n",
       "      <td>No finding</td>\n",
       "      <td>14</td>\n",
       "      <td>R10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2540.0</td>\n",
       "      <td>3072.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           image_id          class_name  class_id rad_id  \\\n",
       "0  50a418190bc3fb1ef1633bf9678929b3          No finding        14    R11   \n",
       "1  21a10246a5ec7af151081d0cd6d65dc9          No finding        14     R7   \n",
       "2  9a5094b2563a1ef3ff50dc5c7ff71345        Cardiomegaly         3    R10   \n",
       "3  051132a778e61a86eb147c7c6f564dfe  Aortic enlargement         0    R10   \n",
       "4  063319de25ce7edb9b1c6b8881290140          No finding        14    R10   \n",
       "\n",
       "      x_min     y_min     x_max     y_max   width  height  \n",
       "0       NaN       NaN       NaN       NaN  2332.0  2580.0  \n",
       "1       NaN       NaN       NaN       NaN  2954.0  3159.0  \n",
       "2  0.332212  0.588613  0.794712  0.783818  2080.0  2336.0  \n",
       "3  0.548611  0.257986  0.699219  0.353819  2304.0  2880.0  \n",
       "4       NaN       NaN       NaN       NaN  2540.0  3072.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(os.path.join(\"train1.csv\"))\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_new = dataset[dataset.class_name!='No finding'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_device():\n",
    "    device = \"cpu\"\n",
    "    return device\n",
    "\n",
    "device=set_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "params = [p for p in model.parameters() if p.requires_grad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_transform():\n",
    "    return A.Compose([ToTensorV2(),], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LungsAnnotationDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, transforms=None):\n",
    "        super().__init__()\n",
    "        self.image_ids = dataframe['image_id'].unique()\n",
    "        self.df = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        image_id = self.image_ids[index]\n",
    "        records = self.df[self.df['image_id'] == image_id]\n",
    "\n",
    "        image = io.imread(f'{self.image_dir}/{image_id}.png')\n",
    "\n",
    "        # Normalize the image\n",
    "        image = image / 255.0  # pixel values are in the range [0, 255]\n",
    "        image = exposure.equalize_hist(image)\n",
    "        image = image.astype('float32')\n",
    "\n",
    "        # If the image has 3 channels already (like RGB), no need to stack, else ensure 3 channels\n",
    "        if image.ndim == 2:  # If the image is grayscale, convert to 3 channels\n",
    "            image = np.stack([image, image, image], axis=-1)\n",
    "\n",
    "        # Ensure the image is in the correct (C, H, W) format\n",
    "        if image.shape[2] == 3:  # Check if image is in (H, W, C)\n",
    "            image = image.transpose(2, 0, 1)  # Convert from (H, W, C) to (C, H, W)\n",
    "\n",
    "        # Get bounding boxes and other details\n",
    "        boxes = records[['x_min', 'y_min', 'x_max', 'y_max']].values\n",
    "\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        area = torch.as_tensor(area, dtype=torch.float32)\n",
    "\n",
    "        labels = records.class_id.values + 1\n",
    "        iscrowd = torch.zeros((records.shape[0],), dtype=torch.int64)\n",
    "\n",
    "        target = {\n",
    "            'boxes': torch.tensor(boxes, dtype=torch.float32),\n",
    "            'labels': torch.tensor(labels, dtype=torch.int64),\n",
    "            'area': area,\n",
    "            'iscrowd': iscrowd\n",
    "        }\n",
    "\n",
    "        # Apply transformations if available (pass normalized boxes)\n",
    "        if self.transforms:\n",
    "            sample = {\n",
    "                'image': image,\n",
    "                'bboxes': target['boxes'],\n",
    "                'labels': labels\n",
    "            }\n",
    "            sample = self.transforms(**sample)\n",
    "            image = sample['image']\n",
    "            target['boxes'] = torch.tensor(sample['bboxes'], dtype=torch.float32)\n",
    "\n",
    "        # Denormalize boxes AFTER transformations (if you need pixel coordinates)\n",
    "        target['boxes'][:, [0, 2]] = target['boxes'][:, [0, 2]] * 512\n",
    "        target['boxes'][:, [1, 3]] = target['boxes'][:, [1, 3]] * 512\n",
    "\n",
    "        return image, target\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    images, targets = zip(*batch)\n",
    "    images = [image.permute(1, 2, 0) if image.shape[0] != 3 else image for image in images]\n",
    "    return torch.stack(images), targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Averager:\n",
    "    def __init__(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "\n",
    "    def send(self, value):\n",
    "        self.current_total += value\n",
    "        self.iterations += 1\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        if self.iterations == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1.0 * self.current_total / self.iterations\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_brands = {\n",
    "    0: 'Aortic enlargement',\n",
    "    1: 'Atelectasis',\n",
    "    2: 'Calcification',\n",
    "    3: 'Cardiomegaly',\n",
    "    4: 'Consolidation',\n",
    "    5: 'ILD',\n",
    "    6: 'Infiltration',\n",
    "    7: 'Lung Opacity',\n",
    "    8: 'Nodule/Mass',\n",
    "    9: 'Other lesion',\n",
    "    10: 'Pleural effusion',\n",
    "    11: 'Pleural thickening',\n",
    "    12: 'Pneumothorax',\n",
    "    13: 'Pulmonary fibrosis'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = LungsAnnotationDataset(dataframe=dataset_new, image_dir='resized_images', transforms=get_valid_transform())\n",
    "val_data_loader = DataLoader(val_dataset, batch_size=20, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220\n"
     ]
    }
   ],
   "source": [
    "print(len(val_data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=[], unexpected_keys=['roi_heads.box_predictor.fc.weight', 'roi_heads.box_predictor.fc.bias'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('x_ray_models/model_fasterRCNN_finetuned.pth', map_location=device), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_predictions_to_json(model, val_data_loader, device, class_brands, filename='phase1_results.json'):\n",
    "    results = {}\n",
    "    \n",
    "    model.to(device)  # Ensure the model is on the same device as the inputs\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, (images, _) in enumerate(val_data_loader):\n",
    "            images = [image.to(device) for image in images]  # Move images to the same device\n",
    "            \n",
    "            outputs = model(images)  # Model and inputs are on the same device\n",
    "            \n",
    "            for i, output in enumerate(outputs):\n",
    "                image_id = f'image_{idx * len(outputs) + i}'  # Generate a unique ID for each image\n",
    "                \n",
    "                pred_boxes = output['boxes']\n",
    "                labels_pred = output['labels']\n",
    "                scores = output['scores'].data.cpu().numpy()\n",
    "\n",
    "                # Filter predicted boxes based on confidence score\n",
    "                valid_indices = scores >= 0.2\n",
    "                boxes_pred = pred_boxes[valid_indices]\n",
    "                labels_pred = labels_pred[valid_indices]\n",
    "\n",
    "                # Convert results to list of dictionaries\n",
    "                pred_results = []\n",
    "                for box, label in zip(boxes_pred, labels_pred):\n",
    "                    box_np = box.detach().cpu().numpy().astype(int).tolist()  # Convert tensor to list\n",
    "                    class_name = class_brands.get(label.item(), 'Unknown')\n",
    "                    pred_results.append({\n",
    "                        'box': box_np,\n",
    "                        'class_label': class_name\n",
    "                    })\n",
    "\n",
    "                results[image_id] = pred_results\n",
    "    \n",
    "    # Write results to a JSON file\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "\n",
    "    print(f\"Results saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_device():\n",
    "    device =  \"cpu\"\n",
    "    return device\n",
    "\n",
    "device=set_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to phase1_results.json\n"
     ]
    }
   ],
   "source": [
    "save_predictions_to_json(model, val_data_loader, device, class_brands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iou_hist = Averager()\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     for images, targets in val_data_loader:\n",
    "#         images = [image.to(device) for image in images]\n",
    "#         targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "#         outputs = model(images)\n",
    "\n",
    "#         for output, target in zip(outputs, targets):\n",
    "#             pred_boxes = output['boxes']\n",
    "#             gt_boxes = target['boxes']\n",
    "\n",
    "#             if pred_boxes.numel() == 0 or gt_boxes.numel() == 0:\n",
    "#                 print(f\"Skipping due to empty predicted or ground truth boxes\")\n",
    "#                 continue\n",
    "\n",
    "#             # Calculate IoU\n",
    "#             iou = box_iou(pred_boxes, gt_boxes)\n",
    "\n",
    "#             # Handle NaN values\n",
    "#             if torch.isnan(iou).any():\n",
    "#                 print(f\"NaN detected in IoU calculation. Skipping this batch.\")\n",
    "#                 continue\n",
    "\n",
    "#             # Append average IoU for each image\n",
    "#             avg_iou = iou.mean().item()\n",
    "#             iou_hist.send(avg_iou)\n",
    "\n",
    "#     avg_iou = iou_hist.value\n",
    "#     print(f\"Validation IoU: {avg_iou:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
